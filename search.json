[{"title":"Me","url":"/2024/03/24/Me/","content":""},{"title":"Hello World","url":"/2024/03/24/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"url":"/2024/04/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E4%B8%80/","content":"实验1.1：基于华为云构建大数据实验环境一、实验描述购买华为云ECS服务器，构建大数据实验环境。\n二、实验目的1.了解华为云基础操作；\n2.购买华为云ECS；\n3.掌握华为云服务器操作；\n三、实验步骤进入控制台选择弹性云服务器ECS开始购买，选择鲲鹏计算系列的买四台\n\n\n接下来进入网络配置\n\n接下来高级配置\n\n同意并购买\n\n打开OBS服务创建桶\n\n复制EndPoint值\nobs.cn-north-4.myhuaweicloud.com\n获取AK&#x2F;SK\n\n结果如下\n\n\n\nUser Name\nAccess Key Id\nSecret Access Key\n\n\n\nwindnegev\nZKM5408Z5O5BOQTDJ1GV\nZ5rvq4gDByybWaB14RymlmBgl3ZoT0uZlXeagC0V\n\n\n启动与关闭服务器\n实验1.2：安装Hadoop及HDFS应用实践实验描述在之前购买的华为云ECS服务器上，搭建Hadoop集群。并使用idea创建maven工程，完成HDFS文件读取实践。\n实验目的1.学习搭建Hadoop集群；\n2.学习创建maven工程；\n3.掌握HDFS文件读写操作。\n实验步骤下载和安装远程登录传输服务器工具Putty工具\n\n\n\nWinSCP工具\n\nHadoop集群搭建传统方法搭建Hadoop用SCP连接\n\n将实验文件传入\n配置服务器间免密访问\n关闭防火墙，生成秘钥\n\n获得公钥\n\n\n\n汇总\n\n每个节点进行vim &#x2F;root&#x2F;.ssh&#x2F;authorized_keys并写入这些公钥\n\n执行ifconfig获得内网IP\n\n192.168.0.95\n192.168.0.158\n192.168.0.83\n192.168.0.177\n\nvim &#x2F;etc&#x2F;hosts编辑hosts文件，加入node1~node4对应IP及node节点名。\n本节点的ip用刚刚查到的内网ip，其他节点的ip用外网ip\n这里node1~4分布对应服务器名称3214\n\n顺利进行无密访问\n\n安装OpenJDK\n将jdk安装包拷贝到&#x2F;usr&#x2F;lib&#x2F;jvm目录下\n\n\n\n在该节点将安装包分发到剩余3个节点\n\n这里出现了一次链接断开，所以改成在windows powershell上连接\n\n\n检查一下，确实传到了\n\n\n在node1~node4四个节点分别执行解压命令\n\n\n\n在node1~node4四个节点上编辑&#x2F;etc&#x2F;profile配置环境变量\n\n\n\n​\t刷新配置文件后的结果\n\n安装Hadoop复制hadoop安装包到&#x2F;home&#x2F;modules下\n\n配置hadoop环境变量\n\n发现之前传入的是docker版，这里重新找到了正确的压缩包并放入\n\n\n完成环境变量设置和参数配置\n配置hadoop core-site.xml文件\n\n配置hdfs-site.xml\n\nyarn-site.xml\n\n配置mapred-sit.xml\n\n配置slaves\n\n分发hadoop包到其余节点\n\nnode1~node4四个节点配置HADOOP的环境变量并刷新\n\n设置权限\n\n在上传安装包的节点执行下列命令\nhadoop namenode -format\n启动hadoop：start-all.sh\n输入jps，发现成功\n\n主节点\n\n\n\n子节点\n\n\n可以发现主节点（Master Node）包含以下关键组件：\n\nNameNode： 负责管理Hadoop文件系统（HDFS）的命名空间和文件块的元数据信息。\nSecondaryNameNode： 负责定期合并编辑日志（edits log）和镜像（image）以及创建检查点（checkpoint），以减少NameNode的工作负载。\nResourceManager： 负责分配资源给各个应用程序的不同任务，以及监控这些资源的使用情况。\n\n而子节点（Slave Node）包含以下组件：\n\nDataNode： 负责存储实际的数据块，并响应客户端或其他节点的读取请求。\nNodeManager： 负责管理每个节点上的资源和任务，接收来自ResourceManager的指令，并协调本地资源的使用。\n\n​\t\t这种设计的主要目的是实现分布式系统的高可用性、可靠性和性能优化。\n​\t\t将文件系统的元数据（由NameNode管理）和实际数据存储（由DataNode管理）分开存储，可以提高系统的可靠性。因为如果数据节点发生故障，只会影响到数据的可用性，而元数据仍然可以由NameNode管理。同时，这种分离还允许数据节点在不同的物理服务器上进行分布，从而提高了系统的可伸缩性。\n​\t\tResourceManager负责整个集群资源的全局调度和监控，而NodeManager负责单个节点的资源管理和任务协调。这种分离允许每个节点上的NodeManager独立管理本地资源，并在需要时向ResourceManager请求更多资源，从而提高了集群资源的利用率和任务的执行效率。\n​\t\tSecondaryNameNode的作用是帮助减轻NameNode的负载，通过定期合并和处理元数据的变更信息，从而减少NameNode的编辑日志和镜像的大小。这可以降低系统维护的成本，并提高整个系统的可靠性和性能。\n方法2：Docker搭建Hadoop安装Docker使用apt命令安装docker\n\n运行sudo docker run hello-world\n\n原因是docker版本不对\n应该还是用yum -y docker-ce来安装\n卸载之前的\n\n安装epel更新源\nyum install -y vim wget epel-release\n设置仓库并稳定\n\n安装\nsudo yum install -y docker-ce docker-ce-cli containerd.io\n\n启动docker\nsudo systemctl start docker\n成！\n\n集群搭建新建文件夹Hadoop，将实验文件压缩包中的Dockerfile以及config文件夹放入\n下载hadoop-3.3.6并放入该文件夹，命名为hadoop-3.3.6.tar.gz\n\n用 docker build . -t hello-hadoop 命令创建image\n\n\nFINISHED表示完成\ndocker image ls\n\n​\tImage列表中存在hadoop了\n在四个终端中分别执行以下命令\n\ndocker run -it -h master –name master hello-hadoop\ndocker run -it -h master –name slave1 hello-hadoop\ndocker run -it -h master –name slave2 hello-hadoop\ndocker run -it -h master –name slave3 hello-hadoop\n\n\n​\t进入docker了\n在四个终端中分别执行 cat &#x2F;etc&#x2F;hosts\n得到列表：\n172.17.0.2      master\n172.17.0.3      slave1\n172.17.0.4      slave2\n172.17.0.5      slave3\n将此列表分别写入四个节点的&#x2F;etc&#x2F;hosts文件，替换master一行\n\n在每个节点分别执行”service ssh restart”重启ssh服务\n\n在master节点执行”ssh slave1”查看是否能够登录到其他节点\n\n将master节点中&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;workers中的localhost替换为以下内容：\n\n在master节点分别执行以下三条命令：\n\ncd &#x2F;usr&#x2F;local&#x2F;hadoop\n\nbin&#x2F;hdfs namenode -format\n\n\nsbin&#x2F;start-all.sh\n\n\n\n出现报错\nvi &#x2F;etc&#x2F;profile 添加内容\n\n\n解决之后再进行上述启动操作\n\n这次成功了\n分别在master界面和slave节点执行命令jps\n\n\n创建maven工程创建项目\n依赖设置\n设置语言环境\n设置java Compiler环境\njava实现HDFS文件读写开放端口\n\n\n​\t\t记得也把端口50010也开放。\n​\t\t之后需要将本地的rsa公钥复制到所有远程节点的authorkeys里面，这样就可以进行免密的ssh链接，这样才可以让本地的java毫无限制的访问远程服务器。\n​\t\t此外为了让本地节点知道远程服务器传来的hostname对应的ip是什么，需要配置本地host文件，将对应内容改为\n编写程序\n出现了log4j没有配置的错误\n加入\n\n报错1\n\n使用这一套流程可以正确重启并检查是否正确配置\nstop-all.sh\nrm -rf $HADOOP_HOME&#x2F;tmp\nhadoop namenode -format\nstart-all.sh\njps\nhdfs dfsadmin -report\n\n这里发现所有节点的hostname都被设置为了localhost这个是个严重的问题，需要去host文件里面把127.0.0.1的相关内容都删掉\n删掉之后使用上面的方式进行重启\n修复完成之后，我们情况hdfs之前的文件\n\n\n结果输出\n\n\n上传成功的文件内容\n\n下载下来的文件内容\n\n"}]